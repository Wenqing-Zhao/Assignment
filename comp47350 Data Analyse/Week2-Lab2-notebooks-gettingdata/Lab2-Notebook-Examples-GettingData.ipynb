{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data\n",
    "This notebook showcases how to download data available on the Internet. We cover most formats the data is typically available in, and learn/practice via example Python code or utilities for getting data. \n",
    "\n",
    "TOPIC1: Getting data from a Web URL: text, HTML, XML, PDF.\n",
    "\n",
    "TOPIC2: Crawling/Scraping data from the Web (entire websites).\n",
    "\n",
    "TOPIC3: Getting data via APIs (JSON format)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOPIC1: Getting data from a Web URL: text, HTML, PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/wenqingzhao/opt/anaconda3/envs/comp47350py38/bin/python\n",
      "sys.version_info(major=3, minor=8, micro=16, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "#To check which Python version and virtual environment this Jupyter Notebook uses\n",
    "print(sys.executable)\n",
    "print(sys.version_info)\n",
    "#print(sys.path)\n",
    "\n",
    "#If you find that Jupyter Notebook does not point to the required virtual environment\n",
    "#remove the venv and re-create the virtual environment using\n",
    "#conda create --name comp47350py37 python=3.7 jupyter\n",
    "#Use 'pip install' to re-install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Twitter' from 'twitter' (/Users/wenqingzhao/opt/anaconda3/envs/comp47350py38/lib/python3.8/site-packages/twitter/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#Import the necessary methods from the \"twitter\" library\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtwitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Twitter, OAuth, TwitterHTTPError, TwitterStream\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#Look at the package structure to understand how to use it\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#print(dir(requests))\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#As an alternative can use '?', same as help() but opens a new window\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#?requests.get\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Twitter' from 'twitter' (/Users/wenqingzhao/opt/anaconda3/envs/comp47350py38/lib/python3.8/site-packages/twitter/__init__.py)"
     ]
    }
   ],
   "source": [
    "#Import all required packages\n",
    "#If you don't have these packages, install using: pip install <package-name>\n",
    "\n",
    "#Import package 'requests'for URL scrapping\n",
    "import requests\n",
    "# Import package for reading csv files \n",
    "import pandas as pd\n",
    "#import package 'beautifulsoup' to extract the content of HTML fields \n",
    "#pip install bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#pip install newspaper3k\n",
    "import newspaper\n",
    "\n",
    "#import package 'feedparser'\n",
    "#Feedparser is a library to parse RSS/XML feeds, these are files with a specific XML structure\n",
    "import feedparser\n",
    "#import package 'json' to parse json objects\n",
    "import json\n",
    "#Import the necessary methods from the \"twitter\" library\n",
    "from twitter import Twitter, OAuth, TwitterHTTPError, TwitterStream\n",
    "\n",
    "#Look at the package structure to understand how to use it\n",
    "#print(dir(requests))\n",
    "\n",
    "#Look at individual functions\n",
    "#help(requests.get)\n",
    "\n",
    "#As an alternative can use '?', same as help() but opens a new window\n",
    "#?requests.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a text file.\n",
    "#Get book \"Alice's Adventures in Wonderland\" from Project Gutenberg, in text format\n",
    "\n",
    "#Give the URL for the file to be downloaded\n",
    "url='https://www.gutenberg.org/files/11/11-0.txt'\n",
    "#Look at the object returned by requests.get()\n",
    "requests_object = requests.get(url)\n",
    "\n",
    "#print(requests_object.content)\n",
    "\n",
    "#Get the content from the downloaded text file\n",
    "text_page = requests_object.text\n",
    "#print(text_page)\n",
    "\n",
    "#Look at the first 500 characters of the book\n",
    "print(text_page[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading from a csv file, into a data frame\n",
    "df = pd.read_csv('MotorInsuranceFraudClaimABTFull.csv')\n",
    "\n",
    "# Check how many rows and columns this dataframe has\n",
    "print(\"number of rows and columns:\", df.shape)\n",
    "\n",
    "# Show first 10 rows of data frame\n",
    "# The rows are indexed starting from 0\n",
    "df.head(10)\n",
    "\n",
    "# Show last 10 rows of data frame\n",
    "# The rows are indexed starting from 0\n",
    "#df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get an HTML file.\n",
    "#Get news article from IrishTimes website.\n",
    "\n",
    "#Give the URL for the file to be downloaded\n",
    "url = \"https://www.irishtimes.com/news/world/covid-pandemic-could-end-this-year-if-vaccine-targets-met-says-who-1.4784483\"\n",
    "\n",
    "#Get the content from the downloaded html file\n",
    "html_page = requests.get(url).text\n",
    "#Look at the format of the html file\n",
    "print(html_page[:1000])\n",
    "\n",
    "#write the content to a file\n",
    "file = open(\"it-news-covid-pandemic-could-end-this-year.html\", \"w\") \n",
    "file.write(html_page)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can download a newsarticle and parse it using the newspaper3k library\n",
    "# https://buildmedia.readthedocs.org/media/pdf/newspaper/latest/newspaper.pdf\n",
    "# newspaper cannot parse all types of html files, for more complex file structure we still need 'beautifulsoup'\n",
    "from newspaper import Article\n",
    "\n",
    "url =\"https://www.irishtimes.com/news/world/covid-pandemic-could-end-this-year-if-vaccine-targets-met-says-who-1.4784483\"\n",
    "article = Article(url)\n",
    "article.download()\n",
    "\n",
    "#print(article.html)\n",
    "\n",
    "article.parse()\n",
    "print(\"Authors:\", article.authors)\n",
    "print(\"Date:\", article.publish_date)\n",
    "print(\"Title:\", article.title)\n",
    "print(\"Text:\", article.text)\n",
    "print(\"URL:\", article.url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use package 'beautifulsoup' to extract the content of HTML fields \n",
    "#Need to know the HTML structure and the tags containing the information we need\n",
    "#To look at the HTML file open it in a text editor, look for the tags that contain headline, subheadline, article body \n",
    "#If you don't have beautifulsoup4 installed, run in shell: conda install beautifulsoup4\n",
    "\n",
    "# Method to parse the structure of an html page using package beautifulsoup.\n",
    "# The code looks for specific tags in the html structure and extracts the content\n",
    "def getArticleDetailsByUrl(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text,\"html.parser\")\n",
    "    #soup.prettify()\n",
    "    \n",
    "    headline = soup.title.string\n",
    "    subheadline = soup.head.find(\"meta\",attrs={\"name\":\"description\"}).get('content')\n",
    "\n",
    "    doc_body = ''\n",
    "    if \"The Irish Times\" in soup.text:\n",
    "        for body_p_tag in soup.article.find_all(\"p\", attrs={\"class\": \"no_name\"}):\n",
    "            doc_body += body_p_tag.get_text() + '\\n'\n",
    "\n",
    "    source = \"Other\"\n",
    "    try:\n",
    "        if \"irishtimes\" in url:\n",
    "            source = \"IrishTimes\"\n",
    "            body_p_tag = soup.article.find(\"div\", attrs={\"class\": \"last_updated\"}).find(\"p\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    first_sentence = doc_body.split(\".\")[0]\n",
    "\n",
    "    return [headline, subheadline, first_sentence, doc_body, source]\n",
    "\n",
    "# Main code that calls our parsing method getArticleDetailsByUrl(url) for specific html pages.\n",
    "if __name__ == '__main__':\n",
    "    article_url = \"https://www.irishtimes.com/news/world/covid-pandemic-could-end-this-year-if-vaccine-targets-met-says-who-1.4784483\"\n",
    "    #print(getArticleDetailsByUrl(article_url))\n",
    "    \n",
    "    print(\"\\nField by field:\\n\")\n",
    "    [headline, subheadline, first_sentence, doc_body, source] = getArticleDetailsByUrl(article_url)\n",
    "    print(\"Headline:\\n\", headline, \"\\n\")\n",
    "    print(\"Subheadline:\\n\", subheadline, \"\\n\")\n",
    "    print(\"First sentence:\\n\", first_sentence, \"\\n\")\n",
    "    print(\"Article body:\\n\", doc_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading and working with an XML file\n",
    "#Get the whole RSS feed for the Irish Times news articles\n",
    "#This is an XML file listing the URLs of individual news articles published online\n",
    "#Need to know the structure of the XML to be able to extract text from specific tags\n",
    "\n",
    "#Parse the XML file to retrieve the URLs for individual news articles.\n",
    "#Parse each article's HTML page\n",
    "\n",
    "def scrapeRSSFeed(rss_feed):\n",
    "    d = feedparser.parse(rss_feed)\n",
    "    #print(d)\n",
    "    #print(d['entries'], \"\\n\")\n",
    "        \n",
    "    for item in d['entries']:\n",
    "        #Extract an article URL\n",
    "        article_url = item['link']\n",
    "        [headline, subheadline, first_sentence, doc_body, source] = getArticleDetailsByUrl(article_url)\n",
    "        print(\"\\nArticle:\", headline, \"\\n\")\n",
    "\n",
    "#Here you have your very own RSS feed reader in a few lines of code.\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #The URL of the XML file\n",
    "    url='https://www.irishtimes.com/rss/irish-times-top-10-stories-1.4019566'\n",
    "    xml_page = requests.get(url).text\n",
    "    \n",
    "    #Look at the structure of the XML file\n",
    "    #To have a proper look, open the XML file with a text editor\n",
    "    print(xml_page[:1000])\n",
    "\n",
    "    # Call the method that parses a given XML file\n",
    "    scrapeRSSFeed(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a PDF file, save it to disk.\n",
    "\n",
    "# Give url of the PDF file\n",
    "url='http://www.greenteapress.com/thinkpython/thinkpython.pdf'\n",
    "# Download the pdf file into request_object\n",
    "request_object = requests.get(url)\n",
    "\n",
    "#PDF is a binary format. Use request.content instead of request.text\n",
    "#Write binary content on your machine's disk in a file named 'thinkpython.pdf'\n",
    "with open(\"thinkpython.pdf\", \"wb\") as pdffile:\n",
    "    # Look at the conent of the file; it looks all gibberish since it is a binary format.\n",
    "    # To make sense of the content, we need tools that can read pdf format and extract it to plain text.\n",
    "    # See next cell for pdftotext tool.\n",
    "    print(request_object.content[:500])\n",
    "    \n",
    "    #Print the content of the request_object to a file named \"thinkpython.pdf\"\n",
    "    pdffile.write(request_object.content)\n",
    "\n",
    "#Check that it downloaded the file to the current directory.\n",
    "#%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdftotext\n",
    "\n",
    "# Make sure you have downloaded the \"thinkpython.pdf\" file in your current folder\n",
    "# http://www.greenteapress.com/thinkpython/thinkpython.pdf\n",
    "\n",
    "# Load your PDF\n",
    "with open('thinkpython.pdf', 'rb') as f:\n",
    "    pdf = pdftotext.PDF(f)\n",
    "    \n",
    "# What kind of object is this?\n",
    "#print(type(pdf))\n",
    "\n",
    "# What are the methods and variables of this object.\n",
    "#print(dir(pdf))\n",
    "\n",
    "# Get more detail about how to use this object\n",
    "# print(help(pdf))\n",
    "\n",
    "# How many pages?\n",
    "print(\"pages:\", len(pdf))\n",
    "\n",
    "# Iterate over all the pages\n",
    "#for page in pdf:\n",
    "#    print(\"\\n=====newpage:=====\\n\", page)\n",
    "\n",
    "# Read some individual pages\n",
    "print(\"Page 0:\\n\", pdf[0])\n",
    "print(\"Page 1:\\n\", pdf[1])\n",
    "\n",
    "# Read all the text into one string\n",
    "string_pdf = \"\\n\\n\".join(pdf)\n",
    "\n",
    "# Print the first 500 characters in the string\n",
    "print(\"\\n\\nThe first 500 symbols in the string:\\n\", string_pdf[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic2: Crawling data from the Web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to using the Python package *requests*, you can use the command line *wget* utility to download an HTML page from a given URL or to download an entire website. If you don't have *wget* on your computer, first install it for your platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *wget* tool is great for crawling entire or parts of websites. It recursively follows URLs up to given depth.\n",
    "The example below downloads a part of the website locally, in a folder named *en.wikipedia.org*. The parameter -l tells wget to what depth it should follow URLs from the original URL. The parameter --no-parent tells wget to not download anything other than the given path. See http://linuxreviews.org/quicktips/wget/ for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crawl the website to depth 1. To stop downloading interrupt the kernel from the menu above.\n",
    "! wget https://en.wikipedia.org/wiki/Main_Page -r -l 1 --no-parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to stop crawling after a short while, otherwise it may fill your hard disk or you will get banned by the website\n",
    "! wget https://www.irishtimes.com -l 1 --no-parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a pure Python crawler we can use the Python *wget* package or the *scrapy* package (scrapy only works with Phyton2.7 though). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic3: Getting data via APIs.\n",
    "### JSON format: \n",
    "JavaScript Object Notation - a text format used widely for web-based resource sharing. Many packages and APIs return data in JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a file named *example.json* using the Python code below to write a given string to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = \"\"\"\n",
    "{\n",
    "    \"glossary\": {\n",
    "        \"title\": \"example glossary\",\n",
    "\t\t\"GlossDiv\": {\n",
    "            \"title\": \"S\",\n",
    "\t\t\t\"GlossList\": {\n",
    "                \"GlossEntry\": {\n",
    "                    \"ID\": \"SGML\",\n",
    "\t\t\t\t\t\"SortAs\": \"SGML\",\n",
    "\t\t\t\t\t\"GlossTerm\": \"Standard Generalized Markup Language\",\n",
    "\t\t\t\t\t\"Acronym\": \"SGML\",\n",
    "\t\t\t\t\t\"Abbrev\": \"ISO 8879:1986\",\n",
    "\t\t\t\t\t\"GlossDef\": {\n",
    "                        \"para\": \"A meta-markup language, used to create markup languages such as DocBook.\",\n",
    "\t\t\t\t\t\t\"GlossSeeAlso\": [\"GML\", \"XML\"]\n",
    "                    },\n",
    "\t\t\t\t\t\"GlossSee\": \"markup\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\"\"\"\n",
    "with open(\"example.json\", \"w\") as file:\n",
    "    file.write(json_string)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run shell command \"cat\" to look at the file\n",
    "# The sign ! tells Jupyter Notebook that the following command is a shell/terminal command.\n",
    "!cat example.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json.load(open('example.json'))\n",
    "#json_data looks like a nested Python dictionary\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can refer to different fields of the json object\n",
    "print(json_data['glossary']['title'])\n",
    "print(json_data['glossary']['GlossDiv']['title'])\n",
    "print(json_data['glossary']['GlossDiv']['GlossList']['GlossEntry']['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below we use an URL called an API endpoint and the *requests* package to get a json file, as we have seen above in getting data from an URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://data.colorado.gov/resource/4ykn-tg5h.json'\n",
    "json_dataset = requests.get(url).text\n",
    "print(len(json_dataset))\n",
    "#Look at the first 500 characters of the json list\n",
    "print(json_dataset[:500])\n",
    "\n",
    "with open(\"data_colorado_gov.json\", \"w\") as file:\n",
    "    file.write(json_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter API\n",
    "\n",
    "You must have a Twitter account and Twitter OAuth credentials available from https://apps.twitter.com/. \n",
    "For now you can use the credentials below, but Twitter may reject too many connections on the same credentials.\n",
    "It is important to create and use your own authentification. The credentials below will be reset after this lab.\n",
    "Create a new application (using your own Twitter credentials) and then generate access tokens. See this tutorial for more details:\n",
    "http://socialmedia-class.org/twittertutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Twitter Search API to get public tweets from the past\n",
    "# Initiate the connection to Twitter API\n",
    "# Twitter API returns data in JSON format\n",
    "\n",
    "# Variables that contains the user credentials to access Twitter API \n",
    "# ACCESS_TOKEN = 'YOUR ACCESS TOKEN\"'\n",
    "# ACCESS_SECRET = 'YOUR ACCESS TOKEN SECRET'\n",
    "# CONSUMER_KEY = 'YOUR API KEY'\n",
    "# CONSUMER_SECRET = 'ENTER YOUR API SECRET'\n",
    "ACCESS_TOKEN = '2839893905-pBXUzdrHCNXyjfPuBpSwxNbH1zyEpRaa2sXK0Jd'\n",
    "ACCESS_SECRET = 'eNtB7YTAfsMhPIQtKji8aQT7zQFpFfDPR2lQ89WKfgI1U'\n",
    "CONSUMER_KEY = 'ZqPrfLpc0znZlz3kW2a22VmUa'\n",
    "CONSUMER_SECRET = 'BHD19T0DmUV2XVvEhUAgvpXMx0nGfxevAtr53NbCd9jQjPyTqn'\n",
    "\n",
    "oauth = OAuth(ACCESS_TOKEN, ACCESS_SECRET, CONSUMER_KEY, CONSUMER_SECRET)\n",
    "twitter = Twitter(auth=oauth)\n",
    "            \n",
    "# Search for latest 100 tweets about \"#biden\"\n",
    "file = open(\"twitter_search_100tweets_hashtag_biden.json\", \"w\") \n",
    "\n",
    "iterator = twitter.search.tweets(q='#biden', result_type='recent', lang='en', count=100)\n",
    "#print(json.dumps(iterator, indent=4))\n",
    "\n",
    "for tweet in iterator['statuses']:\n",
    "    # tweet is a json object\n",
    "    print(tweet)\n",
    "    #only print the text of the tweet out of the json object\n",
    "    print(\"\\n\\\"\", tweet['text'], \"\\\"\\n\")\n",
    "    file.write(json.dumps(tweet)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming previous tweets were saved in a file *twitter_search_100tweets_hashtag_biden.json*, read the file and look at the tweets format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the file saved from last step as an example of working with json\n",
    "with open('twitter_search_100tweets_hashtag_biden.json', 'r') as f:\n",
    "    tweets_file = f.readlines()\n",
    "#print(tweets_file)\n",
    "\n",
    "for line in tweets_file:\n",
    "    #print(line)\n",
    "    try:\n",
    "        # Read in one line of the file, convert it into a json object \n",
    "        tweet = json.loads(line.strip())\n",
    "        #print(tweet)\n",
    "        if 'text' in tweet: # only messages contains 'text' field is a tweet\n",
    "#             print(tweet['id']) # This is the tweet's id\n",
    "#             print(tweet['created_at']) # when the tweet posted\n",
    "#             print(tweet['text']) # content of the tweet\n",
    "                        \n",
    "#             print(tweet['user']['id']) # id of the user who posted the tweet\n",
    "#             print(tweet['user']['name']) # name of the user, e.g. \"Wei Xu\"\n",
    "#             print(tweet['user']['screen_name']) # name of the user account, e.g. \"cocoweixu\"\n",
    "\n",
    "#             hashtags = []\n",
    "#             for hashtag in tweet['entities']['hashtags']:\n",
    "#             \thashtags.append(hashtag['text'])\n",
    "#             print(hashtags)\n",
    "            date = tweet['created_at']\n",
    "            id = tweet['id']\n",
    "            text = tweet['text']\n",
    "            nfollowers = tweet['user']['followers_count']\n",
    "            nfriends = tweet['user']['friends_count']\n",
    "            hashtags = [hashtag['text'] for hashtag in tweet['entities']['hashtags']]\n",
    "            users = [user_mention['screen_name'] for user_mention in tweet['entities']['user_mentions']]\n",
    "            urls = [url['expanded_url'] for url in tweet['entities']['urls']]\n",
    "    \n",
    "            media_urls = []\n",
    "            if 'media' in tweet['entities']:\n",
    "                media_urls = [media['media_url'] for media in tweet['entities']['media']]\t  \n",
    "    \n",
    "            print([date, id, text, hashtags, users, urls, media_urls, nfollowers, nfriends])\n",
    "    except:\n",
    "        # read in a line that is not in JSON format (sometimes error occured)\n",
    "        print(\"JSON error!!!\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Using Twitter Streaming API to stream tweets in real-time\n",
    "#Gather all tweets containing a given keyword\n",
    "#You can also gather all tweets of given user, check Twitter Streaming API details.\n",
    "\n",
    "# Variables that contains the user credentials to access Twitter API \n",
    "# ACCESS_TOKEN = 'YOUR ACCESS TOKEN\"'\n",
    "# ACCESS_SECRET = 'YOUR ACCESS TOKEN SECRET'\n",
    "# CONSUMER_KEY = 'YOUR API KEY'\n",
    "# CONSUMER_SECRET = 'ENTER YOUR API SECRET'\n",
    "ACCESS_TOKEN = '2839893905-pBXUzdrHCNXyjfPuBpSwxNbH1zyEpRaa2sXK0Jd'\n",
    "ACCESS_SECRET = 'eNtB7YTAfsMhPIQtKji8aQT7zQFpFfDPR2lQ89WKfgI1U'\n",
    "CONSUMER_KEY = 'ZqPrfLpc0znZlz3kW2a22VmUa'\n",
    "CONSUMER_SECRET = 'BHD19T0DmUV2XVvEhUAgvpXMx0nGfxevAtr53NbCd9jQjPyTqn'\n",
    "\n",
    "oauth = OAuth(ACCESS_TOKEN, ACCESS_SECRET, CONSUMER_KEY, CONSUMER_SECRET)\n",
    "\n",
    "file = open(\"twitter_stream_10tweets_hashtag_biden.json\", \"w\") \n",
    "\n",
    "# Initiate the connection to Twitter Streaming API\n",
    "twitter_stream = TwitterStream(auth=oauth)\n",
    "\n",
    "# Get a sample of the public data published on Twitter in real-time\n",
    "#iterator = twitter_stream.statuses.sample()\n",
    "# Get a sample of tweets in English, containing #biden\"\n",
    "iterator = twitter_stream.statuses.filter(track=\"#biden\", language=\"en\")\n",
    "\n",
    "# Print each tweet in the stream to the screen \n",
    "# Here we set it to stop after getting 10 tweets. \n",
    "# You don't have to set it to stop, but can continue running \n",
    "# the Twitter API to collect data for days or even longer. \n",
    "# Please read the APIs T&C.\n",
    "tweet_count = 10\n",
    "\n",
    "for tweet in iterator:\n",
    "    tweet_count -= 1\n",
    "    # Twitter Python Tool wraps the data returned by Twitter \n",
    "    # as a TwitterDictResponse object.\n",
    "    # We convert it back to the JSON format to print/score\n",
    "    #print(json.dumps(tweet))  \n",
    "    file.write(json.dumps(tweet)+\"\\n\")\n",
    "\n",
    "    # The command below will do pretty printing for JSON data, try it out\n",
    "    print(json.dumps(tweet, indent=4))\n",
    "       \n",
    "    if tweet_count <= 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
